{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running PyTorch vs TensorFlow on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for TensorFlow\n",
    "\n",
    "If a TensorFlow operation has both CPU and GPU implementation, the GPU devices will be given priority, unless specified otherwise. \n",
    "\n",
    "e.g., on a system with devices `cpu:0` and `gpu:0`, `gpu:0` will automatically be selected to run `matmul`.\n",
    "\n",
    "Below are three examples:\n",
    "\n",
    "1. Generate matrices *and* run `matmul` all on the CPU.\n",
    "\n",
    "2. Run `matmul` on a single GPU, but with matrix generation done on CPU (ie. with numpy). Note that this is not recommended, as the overhead to send data to the GPU to be processed at each iteration does not provide any speedup from the case where everything is run on the CPU.\n",
    "\n",
    "3. Generate tensors that are automatically assigned to the GPU memory (ie. the default for TensorFlow) and then run `matmul` on these tensors. In this case everything will be done on the GPU and speedup will be considerable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.9692511558532715 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1 (everything done on CPU)\n",
    "\n",
    "N = 100\n",
    "ntimes = 1000\n",
    "tstart = time.time()\n",
    "\n",
    "for _ in range(ntimes):\n",
    "    with tf.device('/cpu:0'):\n",
    "        a = np.random.randn(N,N)\n",
    "        b = np.random.randn(N,N)\n",
    "        c = tf.matmul(a,b)\n",
    "\n",
    "tend = time.time()\n",
    "print('Time Elapsed: {} s\\n'.format(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 1.0640838146209717 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2 (matrices generated on CPU, multiplication done on GPU)\n",
    "\n",
    "N = 100\n",
    "ntimes = 1000\n",
    "tstart = time.time()\n",
    "\n",
    "for _ in range(ntimes):\n",
    "    a = np.random.randn(N,N)\n",
    "    b = np.random.randn(N,N)\n",
    "    c = tf.matmul(a,b)\n",
    "\n",
    "tend = time.time()\n",
    "print('Time Elapsed: {} s\\n'.format(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.25314784049987793 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3 (everything done on GPU)\n",
    "\n",
    "N = 100\n",
    "ntimes = 1000\n",
    "tstart = time.time()\n",
    "\n",
    "for _ in range(ntimes):\n",
    "    a = tf.random.normal([N,N])\n",
    "    b = tf.random.normal([N,N])\n",
    "    c = tf.matmul(a,b)\n",
    "\n",
    "tend = time.time()\n",
    "print('Time Elapsed: {} s\\n'.format(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for PyTorch\n",
    "\n",
    "To utilise the GPU implementation, you must first manually allocate tensors in GPU memory (unlike TensorFlow, PyTorch does not do this automatically). Afterwards, when you perform operations on those tensors, e.g. `matmul`, they will be run on the GPU.\n",
    "\n",
    "To allocate tensors in GPU memory, you must make them of a `torch.cuda` tensor type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.7882390022277832 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1 (everything done on CPU)\n",
    "\n",
    "N = 100\n",
    "ntimes = 1000\n",
    "tstart = time.time()\n",
    "\n",
    "for _ in range(ntimes):\n",
    "    a = np.random.randn(N,N)\n",
    "    b = np.random.randn(N,N)\n",
    "    c = torch.matmul(torch.from_numpy(a), torch.from_numpy(b))\n",
    "\n",
    "tend = time.time()\n",
    "print('Time Elapsed: {} s\\n'.format(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0.037106990814208984 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2 (everything done on GPU)\n",
    "\n",
    "N = 100\n",
    "ntimes = 1000\n",
    "tstart = time.time()\n",
    "\n",
    "for _ in range(ntimes):\n",
    "    a = torch.randn(N,N, device='cuda:0')\n",
    "    b = torch.randn(N,N, device='cuda:0')\n",
    "    c = torch.matmul(a,b)\n",
    "\n",
    "tend = time.time()\n",
    "print('Time Elapsed: {} s\\n'.format(tend-tstart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tfpt_combo_env]",
   "language": "python",
   "name": "conda-env-.conda-tfpt_combo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
